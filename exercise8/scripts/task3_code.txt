The code for detection:

import numpy as np
import cv2
# import cv2.cv as cv
# import pytedsseract
import qrcode
from PIL import Image
from os import listdir
from os.path import isfile, join
import matplotlib.pyplot as plt
import pytesseract
from __future__ import print_function
import pyzbar.pyzbar as pyzbar

# Read images and read text

def decode(im) : 
    # Find barcodes and QR codes
    decodedObjects = pyzbar.decode(im)
 
    # Print results
    for obj in decodedObjects:
        print('Type : ', obj.type)
        print('Data : ', obj.data,'\n')
     
    return decodedObjects
    
# Display barcode and QR code location  
def display(im, decodedObjects):
 
    # Loop over all decoded objects
    for decodedObject in decodedObjects: 
        points = decodedObject.location
 
    # If the points do not form a quad, find convex hull
    if len(points) > 4 : 
        hull = cv2.convexHull(np.array([point for point in points], dtype=np.float32))
        hull = list(map(tuple, np.squeeze(hull)))
    else : 
        hull = points;
     
    # Number of points in the convex hull
    n = len(hull)
 
    # Draw the convext hull
    for j in range(0,n):
        cv2.line(im, hull[j], hull[ (j+1) % n], (0,255,0), 10)
 
    # Display results
    display_bgr(im)
#     cv2.imshow("Results", im);
#     cv2.waitKey(0);

dataset_folder = '/home/matej/programs/ROSImages2/sliki_plot'
image_files = sorted([join(dataset_folder,f) for f in listdir(dataset_folder) if isfile(join(dataset_folder, f)) and f.split('.')[-1]=="jpg"])
for im in image_files:
    print(im)
    im = cv2.imread(im)
    
    corners, ids, _ = cv2.aruco.detectMarkers(im,dict)

    img_out = np.zeros((351,248,3), np.uint8)
    marker_side = 50
    out_pts = np.array([[marker_side/2,img_out.shape[0]-marker_side/2],
                        [img_out.shape[1]-marker_side/2,img_out.shape[0]-marker_side/2],
                        [marker_side/2,marker_side/2],
                        [img_out.shape[1]-marker_side/2,marker_side/2]])

    # print corners
    # print ids
    # print corners[0].shape
    # print ids.shape

    src_points = np.zeros((4,2))
    cens_mars = np.zeros((4,2))
#     print ids
    if not ids is None:
        if len(ids)==4:
#             print ids
            
            for idx in ids:
                cors = np.squeeze(corners[idx[0]-1])
                cen_mar = np.mean(cors,axis=0)
                cens_mars[idx[0]-1]=cen_mar
#               
            cen_point = np.mean(cens_mars,axis=0)
            
            for coords in cens_mars:
                if coords[0]<cen_point[0] and coords[1]<cen_point[1]:
                    src_points[2]=coords
                elif coords[0]<cen_point[0] and coords[1]>cen_point[1]:
                    src_points[0]=coords
                elif coords[0]>cen_point[0] and coords[1]<cen_point[1]:
                    src_points[3]=coords
                else:
                    src_points[1]=coords
                    
#                 src_points[idx[0]-1]=cen_mar
            #     print 'Marker', idx[0], 'is at', cen_mar
#                   cv2.circle(img, tuple(cen_mar), 3, (0,0,255), -1)

            h, status = cv2.findHomography(src_points, out_pts)
            img_out = cv2.warpPerspective(im, h, (img_out.shape[1],img_out.shape[0]))
            
            # Convert the image to grayscale
            img_out = cv2.cvtColor(img_out, cv2.COLOR_BGR2GRAY)
            
            # Cut out everything but the numbers
            img_out = img_out[125:225,50:195,:]
            
            
            decodedObjects = decode(res)
            
            # Convert the image to grayscale
            img_out = cv2.cvtColor(img_out, cv2.COLOR_BGR2GRAY)

            # Option 1 - use ordinairy threshold the image to get a black and white image
#             ret,img_out = cv2.threshold(img_out,100,255,0)

            # Option 1 - use adaptive thresholding
            img_out = cv2.adaptiveThreshold(img_out,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,5)
            
            # Use Otsu's thresholding
#             ret,img_out = cv2.threshold(img_out,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)

            # Visualize the image to see what we are passing to tesseract
            display_gray(img_out)
    
            # Pass some options to tesseract
            config = '--psm 13 outputbase nobatch digits'
            
            # Extract text from image
#             text = pytesseract.image_to_string(img_out, config = config)
            
            # Check and extract data from text
#             print('Extracted>>',text)
            
            # Remove any whitespaces from the left and right
#             text = text.strip()
            
#             # If the extracted text is of the right length
#             if len(text)==2:
#                 x=int(text[0])
#                 y=int(text[1])
#                 print('The extracted datapoints are x=%d, y=%d' % (x,y))
#             else:
#                 print('The extracted text has is of length %d. Aborting processing' % len(text))
            
        else:
            pass
    
        
#     display(res, decodedObjects)
